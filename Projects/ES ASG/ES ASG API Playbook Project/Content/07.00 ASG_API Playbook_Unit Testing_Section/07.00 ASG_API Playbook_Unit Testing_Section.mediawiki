<p>This page was generated from <b>ASG_API Playbook_07.00 Unit Testing_Section_01.06_Published Draft Links Verified.docx</b> on <b>Thu 09/27/2018</b> at <b>16:50:55.24 Eastern Time Zone</b>.</p>
<p>Refer to the three ways to provide feedback on the Wiki Home page.</p>
<p><a href="mailto:ronald.opperman@va.gov;paul.marshall4@va.gov?subject=ASG_API Playbook_07.00 Unit Testing_Section_01.06_Published Draft Links Verified">Send Feedback to this page Via Email</a></p>
<p><a href="https://github.com/department-of-veterans-affairs/ES-ASG/tree/master/Projects/ES%20ASG/ES%20ASG%20API%20Playbook%20Project/Content/07.00%20ASG_API%20Playbook_Unit%20Testing_Section/07.00 ASG_API Playbook_Unit Testing_Section.mediawiki">Edit the Wiki .mediawiki file here.</a></p>
<p><a href=https://github.com/department-of-veterans-affairs/ES-ASG/tree/master/Projects/ES%20ASG/ES%20ASG%20API%20Playbook%20Project/Content/07.00%20ASG_API%20Playbook_Unit%20Testing_Section>Wiki source content can be found here.</a></p>
<p>__TOC__</p>
= Unit Testing Guidelines =

== Unit Testing Approach ==

Following the Test-Driven Development (TDD) approach to API development (Editor’s Note: Reference TDD Section) provides a key benefit of being able to have the API contract defined, mock service created, and test plan with test cases and steps ready before even the full-blown API development is under way. This allows for any API contract and code changes in the future to be fully regression test-ready, reducing the risk to uncover new code defects with frequently changing API contracts and code.

'''Example of the TDD Approach:'''

As an example, a developer is given a task to design and develop a particular System API to interact with the backend relational database and expose the Create/Read/Update/Delete (CRUD) operations in the database via RESTful API operations.

Following the TDD approach to designing and developing the API, the first thing an API developer does is gather the requirements for the API operations and create an API contract (in RAML or Swagger). After the contract is created, a mock API is generated. In Swagger, the NodeJS or Java utility is run to generate source code based on the Swagger contract. In Mulesoft Anypoint Platform, using Design Center, a mock API is created using a designed RAML contract.

In Apache Axis or CXF, an API developer follows a similar approach by designing an API, defining the API contract, and then running an Axis or CXF command-line utility to generate web service stubs. For example, once a web service WSDL is defined based on an API contract definition, a command-line utility in Apache CXF, wsdl2java, can be used to general initial Java class stubs, as defined in this spec: http://cxf.apache.org/docs/wsdl-to-java.html

In NodeJS loopback, a loopback utility to generate NodeJS classes from Swagger specification can be used to create NodeJS controller and corresponding services.

Once the mock service is available for consuming its operations, initial mock-driven unit tests are created using one of the unit testing frameworks. In Java, JUnit is used to create unit test classes based on generated Java objects and methods. In NodeJS, Mocha and Chai unit test frameworks are used. In Mule 4.x, MUnit 4.x unit testing framework is used to create unit test flows and sub-flows, after the initial basic APIKit router and flows from your RAML contract are developed. After the unit tests have been created, they are validated by executing them inside the Integrated Development Environment. An APIKit router is used inside Mule API project to route all incoming HTTP requests to an appropriate process flow inside Mule based on the path defined in the API RAML contract.

The above steps are performed before starting actual API operation implementations. At this stage, an API contract, mock API with operation returning mock data, and unit tests are developed and fully tested. All of the above can be performed in a single “Planning” Sprint, before the development Sprints have started.

What if the Planning Sprint is started and the actual API implementation to expose a relational database using CRUD operations via REST API operations has begun and a new requirement appears? If the change is approved for inclusion, the API contract coding changes would be made, unit tests updated, and the new contract tested. Executing unit tests is relatively quick and efficient using TDD.

Additionally, the above process of making API changes and running regression tests of any API contract changes can be automated. That is, provided the API developer has created a full suite of unit test cases to cover their API code, these unit tests can be executed automatically. Typically, there is a trigger on code check-in to a source code repository, such as GitHub or Subversion, to automatically execute unit tests. Furthermore, functional regression test cases can also be automatically executed once the full API source code is ready for deployment into an Integration, Qa or Production environment.

[[https://github.com/department-of-veterans-affairs/ES-ASG/raw/master/Projects/ES%20ASG/ES%20ASG%20API%20Playbook%20Project/Content/07.00%20ASG_API%20Playbook_Unit%20Testing_Section/media/image1.png|473x259px]]

Note: Image created by Naresh Jain, Agile Faqs

== Agnostic Unit Testing ==

There are programming language and platform specific unit testing frameworks for API development. For Java, a widely used unit testing framework is Junit. For NodeJS, Mocha and Chai. And for Mulesoft, MUnit. Regardless of the unit testing framework, key unit test development guidelines should be followed, including:

* Develop your unit tests to prove API implementation is working correctly as expected. Defect avoidance is the key goal of developing thorough unit tests.
* Ensure that code coverage with your unit tests is close to 100%. In other words, when developing your unit tests, almost every line of your code should be executed when running your unit tests. Test coverage reports are usually provided as part of a unit test framework of choice, or added on as part of the overall test harness.
* Ensure that your unit tests are targeting requirements provided by your target API consumers. In other words, don’t develop unit tests for methods and functions that are not directly relevant to the requirements. Build a traceability report from requirement or user story to a test case and step in your functional and unit test harness.
* Execute unit tests every time your API service operation has been modified, to prevent code regression between API release updates.
* For minor API release updates, execute focused unit test harness specific to the component or operation being changed. For major API release updates, execute full regression testing by running all unit tests for your API.

To develop unit tests, the developer creates test classes in the main codebase associated with each of the key source code components and methods in the API source. For example, if source code is developed to implement a specific API operation which performs an HTTP POST of XML data to the backend system, the corresponding unit test class will be contain the call to the method within the API operation that performs the HTTP POST. As a best practice, the unit test class won’t actually perform a POST to the backend, instead a mock equest and response using a mock unit test framework, such as Mockito or similar, would be performed.

The point of unit tests is not to simulate the behavior of the actual API operation using live operations, but rather to test the source code that has been developed to perform any of the API operations. Therefore, it’s not necessary to make an HTTP POST call in the unit test class, but rather mock the HTTP request and response from the backend service, since the point is not to unit test the backend source code, but rather to unit testing the API source code.

The focus of the unit tests should be on complex components in your API source code, such as security authentication/authorization, complex algorithms, transformations (e.g., XML to JSON), database operations, and service orchestration and business rules. Those components and operations are frequently subject to change due to updated requirements, and hence require extra regression testing via your unit and functional test harnesses.

Again, the target goal should be unit test coverage of most of the source code. Recommended test coverage percentage should be close to 95% of all source code.

An example unit test coverage report looks like this (using JetBrains IDEA as an integrated development environment for Java development):

[[https://github.com/department-of-veterans-affairs/ES-ASG/raw/master/Projects/ES%20ASG/ES%20ASG%20API%20Playbook%20Project/Content/07.00%20ASG_API%20Playbook_Unit%20Testing_Section/media/image2.png|418x328px]]

In Eclipse, a similar test coverage report can be generated by using a built-in “Coverage View”:

[[https://github.com/department-of-veterans-affairs/ES-ASG/raw/master/Projects/ES%20ASG/ES%20ASG%20API%20Playbook%20Project/Content/07.00%20ASG_API%20Playbook_Unit%20Testing_Section/media/image3.png|624x231px|Image result for eclipse test coverage]]

Unit tests are frequently integrated into an automated Continuous Integration (CI) pipeline using Jenkins, Bamboo or TeamCity. The tests are executed during the test stage of the CI pipeline to guarantee that the source code delivered to a given environment is bug-free. To emphasize, it is critical to develop unit tests to cover most of your code to avoid any possibility of code delivered with bugs to QA or Production environments.

A screenshot below shows example Jenkins pipeline stages that execute unit tests, then functional tests, and finally load and security tests as three separate pipeline stages, before final deploy readiness:

[[https://github.com/department-of-veterans-affairs/ES-ASG/raw/master/Projects/ES%20ASG/ES%20ASG%20API%20Playbook%20Project/Content/07.00%20ASG_API%20Playbook_Unit%20Testing_Section/media/image4.png|457x270px]]



A developer needs to be aware whether or not their API is fully covered by the unit tests that they have developed. One recommended approach is to develop a series of unit tests and run the test coverage report, to determine which class, component, or function requires more unit tests to be added to fully cover that piece of code. When using Eclipse or JetBrains, code coverage reports are built into the IDE. With NodeJS, a developer can leverage code coverage Node modules such as “istanbul”.

== Defect Types ==

There are various types of defects that can be identified during unit test executions. Unit tests should aimed at detecting bugs within the logical routines, data type discrepancies, formatting issues (e.g. with dates and numbers), and unexpected results returned from functions and routines. Unit test assertions defined inside unit tests should trigger alerts and failures when a defect is detected. For example, an assertion to expect a date returned in a specific format such as MM/DD/YY should be present in a unit test to detect when a date returned from a function call or database is anincorrect format.

== Unit Test Execution Summary ==

When a series of unit tests are executed in Eclipse, WebStorm or via command line (or in Jenkins), the test result summary should always show each unit test, whether that test passed or failed, and the assertion in the case of failure. An example unit test execution summary is provided below. As a general practice, a CI/CD pipeline responsible for executing unit tests should also show such test execution summary in the management console of the CI stage.

[[https://github.com/department-of-veterans-affairs/ES-ASG/raw/master/Projects/ES%20ASG/ES%20ASG%20API%20Playbook%20Project/Content/07.00%20ASG_API%20Playbook_Unit%20Testing_Section/media/image5.png|624x360px|Image result for jenkins unit test summary]]

== Requirements Traceability ==

Each API developer is responsible for ensuring that their code is covered by the unit tests by running the unit test coverage reports. Another important aspect of developing unit tests and functional tests is to make sure that all test cases and steps are traced back to the original requirements. For example, if an API is developed to support a retrieval of customer profile information from the database, the test plan should show a series of test cases and corresponding test steps to retrieve the profile by unique customer id or search the profile by a set of attributes. Then, a functional test suite built in SoapUI, Postman, Cucumber or similar functional test platform should match the test steps which are part of the test cases which, in turn, can be traced back to the business requirements.

To ensure the requirements traceability, a spreadsheet is typically created that shows the requirements, corresponding test cases and steps.

== MuleSoft MUnit ==

Prior to deploying Mule applications and APIs, unit and functional tests are conducted using MUnit, a native testing framework for Mule. A developer creates the MUnit tests to be executed in the local environment, or in the continuous integration and continuous delivery (CI/CD) settings.

Mule 4.x reorganizes the MUnit assertions into two groupings, but majority of assertions remain the same as in MUnit 3.x. Instead of having separate assertions for True, False, Equals, etc, MUnit 4.x simplifies the assertions for equality or inequality into a single “Assert that” MUnit tool.

“Mock when” MUnit tool becomes quite useful to mock a response or statement when a specific variable or parameter contains a given value. For example, an MUnit developer may want to mock the response when a passed-in parameter equals to a specific value.

[[https://github.com/department-of-veterans-affairs/ES-ASG/raw/master/Projects/ES%20ASG/ES%20ASG%20API%20Playbook%20Project/Content/07.00%20ASG_API%20Playbook_Unit%20Testing_Section/media/image6.png|463x250px]]

To create a new MUnit test, follow the Mule guidelines documented online for MUnit generation. There is a built-in utility within Anypoint Studio to generate MUnit tests. With previous versions of Anypoint Studio, the MUnit Tools for Mule plugin had to be manually downloaded:

[[https://github.com/department-of-veterans-affairs/ES-ASG/raw/master/Projects/ES%20ASG/ES%20ASG%20API%20Playbook%20Project/Content/07.00%20ASG_API%20Playbook_Unit%20Testing_Section/media/image7.png|401x273px]]

However, with the new Anypoint Studio 7.x, MUnit already comes pre-bundled with the Studio.

To generate the MUnit flows, simply select a given flow in your code, right-click and select option to generate MUnit flow:

[[https://github.com/department-of-veterans-affairs/ES-ASG/raw/master/Projects/ES%20ASG/ES%20ASG%20API%20Playbook%20Project/Content/07.00%20ASG_API%20Playbook_Unit%20Testing_Section/media/image8.png|525x194px|https://docs.mulesoft.com/munit/v/1.1/_images/right-click-flow.png]]

This will generate a “skeleton” version of the MUnit flow. Next, custom logic and assertions are added using MUnit tools such as “Assert that”, which is the most frequently using assertion type.

MUnit tests can then be run and debugged directly inside Anypoint Studio, with various test data and conditions.

=== Mocking Features ===

# As per best practices outlined in the previous section, mocking backend system responses from a downstream service or platform is recommended. The goal with MUnit is not to test the backend platform, but to test Mule flows and sub-flows. Therefore, when a Mule flow is making an HTTP Request or a JDBC call to the database, the response from that HTTP service or database is mocked, instead of making the actual call.

To mock a response in MUnit, use the Mock Message Processor. The Mock feature provided by MUnit allows for defined mocked behavior for a message processor. In this case, MUnit replaces the normal behavior of the message processor with the defined behavior. Therefore, how a specific message processor responds when it is called can be modified with a particular set of attributes. Use “Mock when” MUnit tool to enable the mock message processor:

&lt;mock:when messageProcessor=&quot;mule:set-payload&quot;&gt;

&lt;/mock:when&gt;

A mock definition is based on '''''matchers''''', that is, parameters that match features of the desired message processor. Defining a mock solely on the name of the message processor largely limits your scope and actions regarding the mock. For this reason, MUnit allows defining a mock by defining matchers over the value of a message processor’s attributes.

&lt;mock:when messageProcessor=&quot;mule:set-payload&quot;&gt;

&lt;mock:with-attributes&gt;

&lt;mock:with-attribute whereValue=&quot;#['Real Set Payload']&quot; name=&quot;doc:name&quot;/&gt;

&lt;/mock:with-attributes&gt;

&lt;/mock:when&gt;

As many attributes as are deemed necessary are defined to make the mock as representative as possible. When defining an attribute, do so by defining these two parameters in the “with-attribute” XML tag of the message processor:

* '''name''' – the name of the attribute
* '''whereValue''' – the value of the attribute

== Integration with Continuous Integration (CI) / Continuous Deployment (CD) ==

The key here is automation - irrespective of various techniques employed by QA teams, humans are not capable of executing complex test scenarios as consistently, frequently and as quickly as machines can.

* There are tools available to continuously run unit tests whenever a code file is saved on disk.
* There are continuous integration systems to run unit and integration tests when code is committed to local branch or to central repo.
* Test scripts can be scheduled to run every XX minutes continuously to check for certain conditions 24x7.

Unit test harness is typically executed as part of the CI pipeline stage called “Commit Stage”, which is triggered after the code is pulled automatically from a source code repository (e.g. Git or GitHub or SVN) onto a build server and compiled.

It is important to note that to properly integrate a CI platform such as Jenkins, TeamCity, or Bamboo with the unit test automated execution, there needs to be significant configuration work performed on the CI platform side to leverage the Maven plugin. Maven is the best practice framework used within Mule projects to automate project builds. A Maven pom.xml file is created by each API developer owning their Mule project. Inside that pom.xml file, there are instructions on what dependencies to include during the project build, as well as whether unit test coverage is required on build and deploy of the code. One instruction in pom.xml can define the percentage threshold required for the code coverage before the project can be successfully built. Jenkins or a similar CI/CD platform can then use a built-in Maven plugin to execute the pom.xml on a given target project as part of its automated build and deploy stages.

After the source code is pulled and compiled on the CI build server, the next stage in the CI pipeline is to execute the unit tests that have been developed as part of your API development lifecycle.

[[https://github.com/department-of-veterans-affairs/ES-ASG/raw/master/Projects/ES%20ASG/ES%20ASG%20API%20Playbook%20Project/Content/07.00%20ASG_API%20Playbook_Unit%20Testing_Section/media/image9.png|502x183px]]

A Continuous Integration (CI) workflow with a unit test stage is shown below, using Jenkins CI Workflow as an example:

[[https://github.com/department-of-veterans-affairs/ES-ASG/raw/master/Projects/ES%20ASG/ES%20ASG%20API%20Playbook%20Project/Content/07.00%20ASG_API%20Playbook_Unit%20Testing_Section/media/image10.png|594x252px]]

When the unit test stage is completed, the unit test coverage report is automatically generated as part of the CI stage. That report is available for viewing on the CI Management Console, such as inside Jenkins CI Console. Finally, the Regression and Release stages are executed before the final Deploy and Publish stages. The Continuous Delivery automation can be integrated together with the Continuous Integration, in a single CI/CD workflow. CD portion of the automation workflow is dealing with actual deployment of the source code into a virtual container (e.g., Docker or OpenShift Pod) or onto the “bare metal” server instance if virtualization is not used.







