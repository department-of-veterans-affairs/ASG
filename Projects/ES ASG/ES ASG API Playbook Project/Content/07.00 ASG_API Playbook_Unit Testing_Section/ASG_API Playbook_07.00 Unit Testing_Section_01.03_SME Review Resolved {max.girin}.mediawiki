<p>Refer to the three ways to provide feedback on the Wiki Home page.</p>
<p><a href=https://github.com/department-of-veterans-affairs/ES-ASG/tree/master/Projects/ES%20ASG/ES%20ASG%20API%20Playbook%20Project/Content/07.00%20ASG_API%20Playbook_Unit%20Testing_Section>Wiki source content can be found here.</a></p>
<p><a href=mailto:jordan.braunstein@visualintegrator.com;paul.marshall4@va.gov>Send Feedback to this page Via Email</a></p>
<p><a href="https://github.com/department-of-veterans-affairs/ES-ASG/tree/master/Projects/ES%20ASG/ES%20ASG%20API%20Playbook%20Project/Content/07.00%20ASG_API%20Playbook_Unit%20Testing_Section/ASG_API Playbook_07.00 Unit Testing_Section_01.03_SME Review Resolved {max.girin}.mediawiki">Edit the Wiki .mediawiki file here.</a></p>
<p>__TOC__</p>
= Unit Testing Guidelines =

== Unit Testing Approach ==

Following Test-Driven Development (TDD) approach to API development provides a key benefit of being able to have the API contract defined, mock service created, and test plan with test cases and steps ready before even the full blown API development is under way. This allows for any API contract and code changes down the road to be fully regression test-ready, reducing the opportunity to uncover new code defects with frequently changing API contracts and code.

'''Example of the TDD Approach:'''

As an example, as a developer you are given a task to design and develop a particular System API to interact with the backend relational database and expose the Create/Read/Update/Delete (CRUD) operations in the database via RESTful API operations.

Following the TDD approach to designing and developing the API, first thing you do as an API developer is gather the requirements for the API operations and create an API contract (in RAML or Swagger). Once the contract is created, you generate a mock API. In Swagger, you would run a NodeJS or Java utility to generate source code based on the Swagger contract. In Mulesoft Anypoint Platform, using Design Center, you would generate a mock API using an designed RAML contract.

In Apache Axis or CXF, an API developer would follow a similar approach by designing an API, defining the API contract, and then running an Axis or CXF command-line utility to generate web service stubs. For example, once a web service WSDL is defined based on an API contract definition, a command-line utility in Apache CXF, called wsdl2java, can be used to general initial Java class stubs, as defined in this spec: http://cxf.apache.org/docs/wsdl-to-java.html

In NodeJS loopback, a loopback utility to generate NodeJS classes from Swagger specification can be used to create NodeJS controller and corresponding services.

Once the mock service is up and available for consuming its operations, you create initial mock-driven unit tests using one of the unit testing frameworks. In Java, you would use JUnit to create unit test classes based your generated Java objects and methods. In NodeJS, you would use Mocha and Chai unit test frameworks. In Mule 4.x, you would use MUnit 4.x unit testing framework to create unit test flows and sub-flows, once you have developed the initial basic APIKit router and flows from your RAML contract. Once the unit tests have been created, you validate them by executing them inside your Integrated Development Environment. An APIKit router is used inside Mule API project to route all incoming HTTP requests to an appropriate process flow inside Mule based on the path defined in the API RAML contract.

All of the above steps are performed even before you started your actual API operation implementations. At this stage, you already have an API contract, mock API with operation returning mock data, and unit tests developed and fully tested. Notice that all of the above can be performed in a single “Planning” Sprint, before even the actual development Sprints have started.

Next, let’s assume that you have completed your Planning Sprint and started the actual API implementation to expose relational database CRUD operations via the REST API operations. Now, in the middle of your development, a new requirement is provided by the future API consumer to change the database schema of that relational database. You are forced to update your API contract and implementation of the API operation. Without unit tests that you have already developed, you are not able to validate whether all of your API contract changes are reflected correctly in your developed code. On the contract, if you have unit tests already created if following the TDD, you would just re-run your unit tests after any API contract and code changes. Executing unit tests is fairly quick and efficient way to validate any change you have to make to your RAML or Swagger contract and to the underlying codebase.

The above process of making API changes and running regression tests of any API contract changes can be automated. Provided the API developer has created a full suite of unit test cases to cover their API code, these unit tests can be executed automatically. Typically, there is a trigger on code check into a source code repository such as GitHub or Subversion to trigger automated unit test executions. Furthermore, functional regression test cases can also be automatically executed once the full API source code is ready for deployment into an integration, Qa or Production environments.

[[https://github.com/department-of-veterans-affairs/ES-ASG/raw/master/Projects/ES%20ASG/ES%20ASG%20API%20Playbook%20Project/Content/07.00%20ASG_API%20Playbook_Unit%20Testing_Section/media/image1.png|473x259px]]

Note: Image created by Naresh Jain, Agile Faqs

== Agnostic Unit Testing ==

There are different unit testing frameworks used for API development, depending on what programming language and platform has been targeted for hosting the API-driven microservice. For Java, an obvious and widely used unit testing framework is JUnit. For NodeJS, it is Mocha and Chai. For Mulesoft API development, it is MUnit. No matter which unit testing framework is chosen, there are key unit test development guidelines to follow:

* Develop your unit tests to prove API implementation is working correctly as expected. Defect avoidance is the key goal of developing thorough unit tests.
* Ensure that code coverage with your unit tests is close to 100%. In other words, when developing your unit tests, almost every line of your code should be executed when running your unit tests. Test coverage reports are usually provided as part of a unit test framework of choice, or added on as part of the overall test harness.
* Ensure that your unit tests are targeting requirements provided by your target API consumers. In other words, don’t develop unit tests for methods and functions that are not directly relevant to the requirements. Build a traceability report from requirement or user story to a test case and step in your functional and unit test harness.
* Execute unit tests every time your API service operation has been modified, to prevent code regression between API release updates.
* For minor API release updates, execute focused unit test harness specific to the component or operation being changed. For major API release updates, execute full regression testing by running all unit tests for your API.

To develop unit tests, as a developer you would create the test classes in your main codebase associated with each of the key source code components and methods in your API source. For example, you have developed source code to implement a specific API operation to perform an HTTP POST of XML data to the backend system. Your corresponding unit test class will be contain the call to the method within your API operation that performs the HTTP POST. As the best practice, your unit test class won’t actually perform a POST to the backend, instead you would mock the request and response using a mock unit test framework such as Mockito or similar.

The point of unit tests is not simulate the behavior of the actual API operation using live operations, but rather to test the source code that you have written as a developer to perform any of the API operations. Therefore, you don’t need to actually make an HTTP POST call in your unit test class, but rather mock the HTTP request and response from the backend service, since you are not actually unit testing the backend source code, but rather you are unit testing your API source code.

The focus of the unit tests should be on more complex components in your API source code, such as security authentication/authorization, complex algorithms, transformations (e.g., XML to JSON), database operations, and service orchestration and business rules. Those components and operations are frequently subject to change due to updated requirements, and hence require extra regression testing via your unit and functional test harnesses.

The target goal, however, should be unit test coverage of most of your source code. Recommended test coverage percentage should be close to 95% of all source code.

An example unit test coverage report looks like this (using JetBrains IDEA as an integrated development environment for Java development):

[[https://github.com/department-of-veterans-affairs/ES-ASG/raw/master/Projects/ES%20ASG/ES%20ASG%20API%20Playbook%20Project/Content/07.00%20ASG_API%20Playbook_Unit%20Testing_Section/media/image2.png|418x328px]]

In Eclipse, a similar test coverage report can be generated by using a built-in “Coverage View”:

[[https://github.com/department-of-veterans-affairs/ES-ASG/raw/master/Projects/ES%20ASG/ES%20ASG%20API%20Playbook%20Project/Content/07.00%20ASG_API%20Playbook_Unit%20Testing_Section/media/image3.png|624x231px|Image result for eclipse test coverage]]

Unit tests are frequently plugged into automated Continuous Integration (CI) pipeline using Jenkins, Bamboo or TeamCity. The tests are executed during the test stage of the CI pipeline to guarantee that the source code delivered to a given environment is bug-free. Therefore, it is critical to develop your unit tests to cover most of your code to avoid any possibility of code delivered with bugs to QA or Production environments.

A screenshot below shows example Jenkins pipeline stages that execute unit tests, then functional tests, and finally load and security tests as three separate pipeline stages, before final deploy readiness:

[[https://github.com/department-of-veterans-affairs/ES-ASG/raw/master/Projects/ES%20ASG/ES%20ASG%20API%20Playbook%20Project/Content/07.00%20ASG_API%20Playbook_Unit%20Testing_Section/media/image4.png|457x270px]]



A developer needs to be aware whether their API is fully covered by the unit tests that they have developed. Recommended solution is to develop a series of unit tests and run the test coverage report, to determine which class, component, or function requires more unit tests to be added to fully cover that piece of code. With Java, if using Eclipse or JetBrains, code coverage reports are built into the IDE. With NodeJS, a developer can leverage code coverage Node modules such as “istanbul”.

Defect Types

There are various types of defects that can and should be identified during unit test executions. Unit tests should be detecting bugs within the logical routines, data type discrepancies, formatting issues (e.g., with dates and numbers), and incorrect results returned from functions and routines. Unit test assertions defined inside unit tests should trigger alerts and failures when a code bug or defect is detected. For example, an assertion to expect a date returned in a specific format such as MM/DD/YY should be present in a unit test to detect whether a date returned back from a function call or database comes back in incorrect format.

== Unit Test Execution Summary ==

When a series of unit tests are executed in Eclipse, WebStorm or via command line (or in Jenkins), the test result summary should always show each unit test, whether that test passed or failed, and the assertion that failed if a given unit test failed. An example unit test execution summary is provided below. As a general practice, a CI/CD pipeline responsible for executing unit tests should also show such test execution summary in the management console of the CI stage.

[[https://github.com/department-of-veterans-affairs/ES-ASG/raw/master/Projects/ES%20ASG/ES%20ASG%20API%20Playbook%20Project/Content/07.00%20ASG_API%20Playbook_Unit%20Testing_Section/media/image5.png|624x360px|Image result for jenkins unit test summary]]

== Requirements Traceability ==

Each API developer is responsible for ensuring that their code is covered by the unit tests by running the unit test coverage reports. Another important aspect of developing unit tests and functional tests is to make sure that all test cases and steps are traced back to the original requirements. For example, if an API is developed to support a retrieval of customer profile information from the database, the test plan should show a series of test cases and corresponding test steps to retrieve the profile by unique customer id or search the profile by a set of attributes. Then, a functional test suite built in SoapUI, Postman, Cucumber or similar functional test platform should match the test steps which are part of the test cases which, in turn, can be traced back to the business requirements.

To ensure the requirements traceability, a spreadsheet is typically created that shows the requirements, corresponding test cases and steps.

== MuleSoft MUnit ==

Prior to deploying your Mule applications and APIs, conduct unit and functional tests using MUnit, a native testing framework for Mule. As a developer, you would create the MUnit tests to be executed in your local environment, or in your continuous integration and continuous delivery (CI/CD) settings.

Mule 4.x reorganizes the MUnit assertions into two groupings, but majority of assertions remain the same as in MUnit 3.x. Instead of having separate assertions for True, False, Equals, etc, MUnit 4.x simplifies the assertions for equality or inequality into a single “Assert that” MUnit tool.

“Mock when” MUnit tool becomes quite useful to mock a response or statement when a specific variable or parameter contains a given value. For example, as an MUnit developer you may want to mock the response when a passed-in parameter equals to a specific value.

[[https://github.com/department-of-veterans-affairs/ES-ASG/raw/master/Projects/ES%20ASG/ES%20ASG%20API%20Playbook%20Project/Content/07.00%20ASG_API%20Playbook_Unit%20Testing_Section/media/image6.png|463x250px]]

To create a new MUnit test, follow the Mule guidelines documented online for MUnit generation. There is a built-in utility within Anypoint Studio to generate MUnit tests. With previous versions of Anypoint Studio, you would had to manually download and install the MUnit Tools for Mule plugin:

[[https://github.com/department-of-veterans-affairs/ES-ASG/raw/master/Projects/ES%20ASG/ES%20ASG%20API%20Playbook%20Project/Content/07.00%20ASG_API%20Playbook_Unit%20Testing_Section/media/image7.png|401x273px]]

However, with the new Anypoint Studio 7.x, MUnit already comes pre-bundled with the Studio.

To generate the MUnit flows, simply select a given flow in your code, right-click and select option to generate MUnit flow:

[[https://github.com/department-of-veterans-affairs/ES-ASG/raw/master/Projects/ES%20ASG/ES%20ASG%20API%20Playbook%20Project/Content/07.00%20ASG_API%20Playbook_Unit%20Testing_Section/media/image8.png|525x194px|https://docs.mulesoft.com/munit/v/1.1/_images/right-click-flow.png]]

This will generate a “skeleton” version of the MUnit flow. Next, you would add custom logic and assertions using MUnit tools such as “Assert that”, which is the most frequently using assertion type.

You can then run and debug your MUnit tests directly inside Anypoint Studio, with various test data and conditions.

=== Mocking Features ===

# As per best practices outlined in the previous section, mocking backend system responses from a downstream service or platform is recommended. Remember that as a developer, your goal with MUnit is not to test your backend platform, but to test your own Mule flows and sub-flows. Therefore, when your Mule flow is making an HTTP Request or a JDBC call to the database, you would want to mock the response from that HTTP service or database, instead of making the actual call.

To mock a response in MUnit, you would use the Mock Message Processor. The Mock feature provided by MUnit allows you to define mocked behavior for a message processor. In this case, MUnit replaces the normal behavior of the message processor with the behavior you define. Therefore you can modify how a specific message processor responds when it is called with a particular set of attributes. You would use “Mock when” MUnit tool to enable the mock message processor:

&lt;mock:when messageProcessor=&quot;mule:set-payload&quot;&gt;

&lt;/mock:when&gt;

A mock definition is based on '''''matchers''''', that is, parameters that match features of the desired message processor. Defining a mock solely on the name of the message processor largely limits your scope and actions regarding the mock. For this reason, MUnit allows you to define a mock by defining matchers over the value of a message processor’s attributes.

&lt;mock:when messageProcessor=&quot;mule:set-payload&quot;&gt;

&lt;mock:with-attributes&gt;

&lt;mock:with-attribute whereValue=&quot;#['Real Set Payload']&quot; name=&quot;doc:name&quot;/&gt;

&lt;/mock:with-attributes&gt;

&lt;/mock:when&gt;

You can define as many attributes as you deem necessary to make the mock as representative as possible. When defining an attribute, you do so by defining these two parameters in the “with-attribute” XML tag of the message processor:

* '''name''' – the name of the attribute
* '''whereValue''' – the value of the attribute

== Integration with Continuous Integration (CI) / Continuous Deployment (CD) ==

The key here is automation - irrespective of various techniques employed by QA teams, humans are not capable of executing complex test scenarios as consistently, frequently and as quickly as machines can.

* There are tools available to continuously run unit tests whenever a code file is saved on disk.
* There are continuous integration systems to run unit and integration tests when code is committed to local branch or to central repo.
* Test script can be scheduled to run every XX minutes continuously to check for certain conditions 24x7.

Unit test harness is typically executed as part of the CI pipeline stage called “Commit Stage”, which is triggered after the code is pulled automatically from a source code repository (e.g., Git or GitHub or SVN) onto a build server and compiled.

It is important to note that to properly integrate a CI platform such as Jenkins, TeamCity, or Bamboo with the unit test automated execution, there needs to be significant configuration work performed on the CI platform side to leverage the Maven plugin. Maven is the best practice framework used within Mule projects to automate project builds. A Maven pom.xml file is created by each API developer owning their Mule project. Inside that pom.xml file, there are instructions on what dependencies to include during the project build, as well as whether unit test coverage is required on build and deploy of the code. One instruction in pom.xml can define the percentage threshold required for the code coverage before the project can be successfully built. Jenkins or a similar CI/CD platform can then use a built-in Maven plugin to execute the pom.xml on a given target project as part of its automated build and deploy stages.

After the source code is pulled and compiled on the CI build server, the next stage in the CI pipeline is to execute the unit tests that you have developed as part of your API development lifecycle.

[[https://github.com/department-of-veterans-affairs/ES-ASG/raw/master/Projects/ES%20ASG/ES%20ASG%20API%20Playbook%20Project/Content/07.00%20ASG_API%20Playbook_Unit%20Testing_Section/media/image9.png|502x183px]]

A Continuous Integration (CI) workflow with a unit test stage is shown below, using Jenkins CI Workflow as an example:

[[https://github.com/department-of-veterans-affairs/ES-ASG/raw/master/Projects/ES%20ASG/ES%20ASG%20API%20Playbook%20Project/Content/07.00%20ASG_API%20Playbook_Unit%20Testing_Section/media/image10.png|594x252px]]

When the unit test stage is completed, the unit test coverage report is automatically generated as part of the CI stage. That report is available for viewing on the CI Management Console, such as inside Jenkins CI Console. Finally, the Regression and Release stages are executed before the final Deploy and Publish stages. The Continuous Delivery automation can be integrated together with the Continuous Integration, in a single CI/CD workflow. CD portion of the automation workflow is dealing with actual deployment of the source code into a virtual container (e.g., Docker or OpenShift Pod) or onto the “bare metal” server instance if virtualization is not used.






